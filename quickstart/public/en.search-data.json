{"/cantaloupe/":{"data":{"cantaloupe-notes#Cantaloupe notes":"Cantaloupe notes Cantaloupe is an open-source dynamic image server for the on-demand generation of derivatives of high-resolution source images, that work with the IIIF Image API (International Image Interoperability Framework).","containerize-cantaloupe-server#Containerize Cantaloupe server":"1. Set up project cantaloupe-server/ |-cantaloupe-5.0.6/ |-images/ |-Dockerfile Download cantaloupe-5.0.6 from https://cantaloupe-project.github.io. Don’t forget to customize the cantaloupe.properties file like this :\nFilesystemSource.BasicLookupStrategy.path_prefix = images/ 2. Write Dockerfile # Use the official image as a parent image (Java 21) FROM openjdk:21 # Set the working directory WORKDIR /Users/benoitboidin/Desktop/s10_info/projet_fin_etudes/m2_pfe_webgpu/sandbox/cantaloupe_server/ # Copy the current directory contents into the container at /usr/src/cantaloupe COPY . . # Make port 8182 available to the world outside this container (localhost:8182) EXPOSE 8182 # Run the application when the container launches CMD [\"java\", \"-Dcantaloupe.config=cantaloupe-5.0.6/cantaloupe.properties\", \"-Xmx2g\", \"-jar\", \"cantaloupe-5.0.6/cantaloupe-5.0.6.jar\"] 2. Build image Open terminal and go to the directory, then\ndocker build -t cantaloupe . 3. Run container Open terminal and go to the directory, then\ndocker run -p 8182:8182 cantaloupe Argument -p 8182:8182 make the container available to host from port.","tutorial#Tutorial":"Requires Java 11+\ndownload Cantaloupe: https://cantaloupe-project.github.io/ duplicate cantaloupe.properties.sample rename the duplicated file cantaloupe.properties in the mentioned file, change FilesystemSource.BasicLookupStrategy.path_prefix to the path of the image directory start the server with java -Dcantaloupe.config=/path/to/cantaloupe.properties -Xmx2g -jar cantaloupe-5.0.6.jar URLs to test the server:\nhttp://localhost:8182/iiif/3/tux.png/info.json http://localhost:8182/iiif/3/tux.png/0,0,800,800/max/0/default.jpg ","use-cantaloupe-api#Use Cantaloupe API":"See: https://iiif.io/api/image/3.0/#4-image-requests\n/cantaloupe/iiif/3/{identifier}/{region}/{size}/{rotation}/{quality}.{format} Parameters identifier region size rotation quality format image file path full max n color jpg square ^max !n gray tif x,y,w,h w, bitonal png pct:x,y,w,h ^w, default gif ,h jp2 ^,h pdf pct:n webp ^pct:n w,h ^w,h !w,h ^!w,h Order of implementation: Region THEN Size THEN Rotation THEN Quality THEN Format"},"title":"Cantaloupe"},"/coco_format/":{"data":{"annotations-structure#Annotations structure":"","base-classes#Base classes":"COCO dataset (Common Objects in Context)Back to README\nCOCO is a large-scale object detection, segmentation, and captioning dataset. COCO has several features:\nObject segmentation Recognition in context Superpixel stuff segmentation 330K images (\u003e200K labeled) 1.5 million object instances 80 object categories 91 stuff categories 5 captions per image 250,000 people with keypoints There are three subset: train and val, that are annotated, and test, which is provided without annotations. This last one is meant to participate in COCO evaluation by submiting prediction on their website.\nTypes dataset COCO dataset has 6 types of annotations (https://cocodataset.org/#format-data):\nobject detection keypoints detection stuff segmentation panoptic segmentation densepose image captioning Annotations structure Every type of dataset has a common structure:\n{ \"info\": info, \"images\": [image], \"annotations\": [annotation], \"licenses\": [license], } info{ \"year\": int, \"version\": str, \"description\": str, \"contributor\": str, \"url\": str, \"date_created\": datetime, } image{ \"id\": int, \"width\": int, \"height\": int, \"file_name\": str, \"license\": int, \"flickr_url\": str, \"coco_url\": str, \"date_captured\": datetime, } license{ \"id\": int, \"name\": str, \"url\": str, } Specific structure for object detection:\nannotation{ \"id\": int, \"image_id\": int, \"category_id\": int, \"segmentation\": RLE or [polygon], \"area\": float, \"bbox\": [x,y,width,height], \"iscrowd\": 0 or 1, } categories[{ \"id\": int, \"name\": str, \"supercategory\": str, }] Other formats can be found in the official documentation (link above).\nBase classes Interesting classes: boat, kite.\n{1: 'person', 2: 'bicycle', 3: 'car', 4: 'motorcycle', 5: 'airplane', 6: 'bs', 7: 'train', 8: 'trck', 9: 'boat', 10: 'traffic light', 11: 'fire hydrant', 12: 'stop sign', 13: 'parking meter', 14: 'bench', 15: 'bird', 16: 'cat', 17: 'dog', 18: 'horse', 19: 'sheep', 20: 'cow', 21: 'elephant', 22: 'bear', 23: 'zebra', 24: 'giraffe', 25: 'backpack', 26: 'mbrella', 27: 'handbag', 28: 'tie', 29: 'sitcase', 30: 'frisbee', 31: 'skis', 32: 'snowboard', 33: 'sports ball', 34: 'kite', 35: 'baseball bat', 36: 'baseball glove', 37: 'skateboard', 38: 'srfboard', 39: 'tennis racket', 40: 'bottle', 41: 'wine glass', 42: 'cp', 43: 'fork', 44: 'knife', 45: 'spoon', 46: 'bowl', 47: 'banana', 48: 'apple', 49: 'sandwich', 50: 'orange', 51: 'broccoli', 52: 'carrot', 53: 'hot dog', 54: 'pizza', 55: 'dont', 56: 'cake', 57: 'chair', 58: 'coch', 59: 'potted plant', 60: 'bed', 61: 'dining table', 62: 'toilet', 63: 'tv', 64: 'laptop', 65: 'mose', 66: 'remote', 67: 'keyboard', 68: 'cell phone', 69: 'microwave', 70: 'oven', 71: 'toaster', 72: 'sink', 73: 'refrigerator', 74: 'book', 75: 'clock', 76: 'vase', 77: 'scissors', 78: 'teddy bear', 79: 'hair drier', 80: 'toothbrsh'} ","coco-dataset-common-objects-in-context#COCO dataset (Common Objects in Context)":"","types-dataset#Types dataset":""},"title":"coco_format"},"/data_augmentation/":{"data":{"data-augmentation-techniques#Data augmentation techniques":"","geometry-based-classic#Geometry-based (classic)":"","pixel-based-classic#Pixel-based (classic)":"Data augmentation techniquesBack to README\nSources:\nhttps://www.sciencedirect.com/science/article/pii/S2590005622000911\nhttps://en.wikipedia.org/wiki/Data_augmentation\nUsed in YOLOX Mixup Mixup is a data augmentation technique that combines two images and their labels to create a new image and label. The new image is a linear combination of the two original images, and the new label is a linear combination of the two original labels. This technique is used in YOLOX to improve the performance of the model.\nExample:\nMosaic Mosaic is a data augmentation technique that combines four images and their labels to create a new image and label. The new image is a combination of the four original images, and the new label is a combination of the four original labels. This technique also is used in YOLOX.\nHSV (probably in YOLOX) HSV stands for Hue, Saturation, and Value. It is a color space that represents colors in terms of their hue, saturation, and value. This technique can be used to alter image quality, brightness, and contrast, therefore creating new images for training.\nExample:\nGeometry-based (classic) Rotation, flipping, cropping, translation Pixel-based (classic) Brightness, contrast, saturation, hue Noise, salt and pepper noise, speckle noise Noise:\nBlur (gaussian):\nAlso see radial blur, motion blur, etc.\nAbout convolutions: https://medium.com/@bdhuma/6-basic-things-to-know-about-convolution-daef5e1bc411","used-in-yolox#Used in YOLOX":""},"title":"data_augmentation"},"/fiftyone/":{"data":{"fiftyone-the-open-source-tool-for-building-high-quality-datasets-and-computer-vision-models#FiftyOne: The open-source tool for building high-quality datasets and computer vision models":"","importing-a-dataset-known-formats#Importing a dataset: known formats":"","plugins#Plugins":"FiftyOne: The open-source tool for building high-quality datasets and computer vision modelsBack to README\nFiftyOne is a Python package that provides a flexible, intuitive interface for:\nexploring dataset filtering and sorting samples detecting similar images detecting detection errors suggesting improvements (FiftyOne Brain) We use it to have an overview of the different datasets we use, to preprocess them, and to visualize the results of our models.\nImporting a dataset: known formats COCO dataset See coco_format\nVOCDetectionDataset \u003cdataset_dir\u003e/ data/ \u003cuuid1\u003e.\u003cext\u003e \u003cuuid2\u003e.\u003cext\u003e ... labels/ \u003cuuid1\u003e.xml \u003cuuid2\u003e.xml ... For further details, see https://docs.voxel51.com/user_guide/dataset_creation/datasets.html#vocdetectiondataset\nKITTIDetectionDataset \u003cdataset_dir\u003e/ data/ \u003cuuid1\u003e.\u003cext\u003e \u003cuuid2\u003e.\u003cext\u003e ... labels/ \u003cuuid1\u003e.txt \u003cuuid2\u003e.txt ... For further details, see https://docs.voxel51.com/user_guide/dataset_creation/datasets.html#kittidetectiondataset\nTFObjectDetectionDataset \u003cdataset_dir\u003e/ tf.records-?????-of-????? For further details, see https://docs.voxel51.com/user_guide/dataset_creation/datasets.html#tfobjectdetectiondataset\nCVATImageDataset \u003cdataset_dir\u003e/ data/ \u003cuuid1\u003e.\u003cext\u003e \u003cuuid2\u003e.\u003cext\u003e ... labels.xml For further details, see https://docs.voxel51.com/user_guide/dataset_creation/datasets.html#cvatimagedataset\nPlugins One need to install the fiftyone desktop app before installing plugins."},"title":"fiftyone"},"/yolox/":{"data":{"data-augmentation#Data augmentation":"","models-formats#Models formats":"","structure#Structure":"","training-on-custom-dataset#Training on custom dataset":"YOLOXBack to README\nStructure YOLO networks are composed of three parts: backbone, neck, and head. The backbone extracts features from the input image, the neck restores spatial information, and the head predicts the bounding boxes and class probabilities.\nThe YOLOX network introduce a new decoupled head. Indeed, earlier version of YOLO networks predict the bounding boxes and class probabilities in a single step. The decoupled head predicts the bounding boxes and class probabilities in two separate steps. This allows the network to predict the bounding boxes and class probabilities more accurately.\nYOLOX structure differs betwen models (YOLOX-S, YOLOX-M, YOLOX-L, YOLOX-X…). Main structure is showned below:\nCBL (Convolutional Block Layer): extract features from the input image.\nSources:\nhttps://www.researchgate.net/figure/Schematic-of-3DSFCNN-CBL-ABL-and-RBL-are-represented-from-top-to-bottom-respectively_fig3_340557387 https://deci.ai/model-zoo/yolox/ https://learnopencv.com/yolox-object-detector-paper-explanation-and-custom-training/#Strong-Data-Augmentation-in-YOLOX Models formats Models are available in different formats:\nonnx (open neural network exchange), for deployment pth, for training (serialized PyTorch state dictionnary, can be resumed) To use YOLOX demo, you need the .pth model file.\nData augmentation Data augmentation in YOLOX:\nmosaic mixup HSV? YOLOX authors say that they don’t need to use ImageNet pre-training anymore after applying these augmentation methods.\nCode seem to augment data with HSV channel (data_augment.py).\nSource :YOLOX data_augment.py script, https://aicurious.io/blog/2021-07-28-yolox\nTraining on custom dataset YOLOX currently support COCO format and VOC format.\nhttps://yolox.readthedocs.io/en/latest/train_custom_data.html\n“Except special cases, we always recommend to use our COCO pretrained weights for initializing the model.”\n“As YOLOX is an anchor-free detector with only several hyper-parameters, most of the time good results can be obtained with no changes to the models or training settings. We thus always recommend you first train with all default training settings.”\nIn order to train the model, the dataset must have the following structure (even if the dataset has nothing to do with 2017): dataset/ ├── annotations/ │ ├── instances_train2017.json │ ├── instances_test2017.json │ └── instances_val2017.json ├── train2017/ ├── test2017/ └── val2017/ Then you must modify (or create a copy) of the yolox/exps/examples/custom/yolox_s.py (or other model) file to match the dataset structure and set hyperparameters. Automatically set the dimensions parameters: # Error with float conversion to int? # self.input_size = int(((max_height-min_height)/2+min_height)//32*32, ((max_width-min_width)/2+min_width)//32*32) # self.test_size = int(((max_height-min_height)/2+min_height)//32*32, ((max_width-min_width)/2+min_width)//32*32) # self.random_size = int(((max_height-min_height)/2+min_height)//32, ((max_width-min_width)/2+min_width)//32) Finally, run the following command: python tools/train.py -f exps/example/custom/yolox_s.py -d 1 -b 4 --fp16 -o -c ../trained_models/yolox_s.pth Preprocessing dataset There are two ways to train YOLOX on a custom dataset:\nmulti-scale training (slower) fixed-size training Hyper-parameters are specified here: https://github.com/Megvii-BaseDetection/YOLOX/blob/main/docs/manipulate_training_image_size.md\nAbout multiscaling: https://mmyolo.readthedocs.io/en/latest/recommended_topics/training_testing_tricks.html\nTo summarize, it is not mandatory to resize images to a fixed size before training. YOLOX can handle images of different sizes.\nEvaluation Average Precision (AP): AP% AP at IoU=.50:.05:.95 (primary challenge metric) APIoU=.50% AP at IoU=.50 (PASCAL VOC metric) APIoU=.75% AP at IoU=.75 (strict metric) AP Across Scales: APsmall% AP for small objects: area \u003c 322 APmedium% AP for medium objects: 322 \u003c area \u003c 962 APlarge% AP for large objects: area \u003e 962 Average Recall (AR): ARmax=1% AR given 1 detection per image ARmax=10% AR given 10 detections per image ARmax=100% AR given 100 detections per image AR Across Scales: ARsmall % AR for small objects: area \u003c 322 ARmedium% AR for medium objects: 322 \u003c area \u003c 962 ARlarge% AR for large objects: area \u003e 962 In COCO, there are more small objects than large objects.\nSpecifically: approximately 41% of objects are small (area \u003c 322), 34% are medium (322 \u003c area \u003c 962), and 24% are large (area \u003e 962).\nArea is measured as the number of pixels in the segmentation mask.","yolox#YOLOX":""},"title":"yolox"}}